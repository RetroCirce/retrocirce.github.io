<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Large-scale Contrastive Language-Audio Petraining</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="config/bootstrap.min.css" />
    <link rel="stylesheet" href="config/ionicons.css" />
    <link rel="stylesheet" href="config/theme.css" />
    <link rel="stylesheet" href="config/template3.css" />
  </head>
<body data-spy="scroll" data-target="#navbar1" data-offset="60">
<div class="container-fluid" style="min-width:1300px;">
    <div class="row">
        <div class="col-12 p-0 bg-primary vh-40 py-5">
            <div class="container" style="min-width:1300px;">
                <div class="row">
                    <div class="col-xl-12 col-lg-12 py-2 text-light">
                        <h2 class="display-5 mb-3">
                            Large Scale Contrastive Language-Audio Pretraining <br> with Feature Fusion and Keyword-to-Caption Augmentation
                        </h2>
                        <p class="lead mb-3">
                            In this page, we give additional introductions to our submission paper of ICASSP: <br>       
                            (1) Some detail information of the proposed dataset LAION-Audio-630K, and training settings during the experiment stage. <br>
                            (2) Some examples of keyword-to-caption augmentation by the T5 text generation model. <br>
                            (3) Some additional experiments and analysis, which are not the main part of the paper but contributes to our conclusion. <br><br>
                            Due to the page limit, we are not able to include above information in the paper. We appreciate the checking of this appendix page if reviewers find it avaiable and valuable to know more information about the submission.
                            </vr>
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<nav class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary" id="navbar1">
    <div class="container" style="min-width:1300px;">
        <a class="navbar-brand mr-1 mb-1 mt-0" href="#">Appendix</a>
    </div>  
</nav> 
<main>
    <section id="synthesis demo" class="pb-1">
        <div class="container" style="min-width:1300px;">
            <div class="row">
                <div class="col-md-12">
                    <div class="pb-1">
                        <p class="lead text-primary"> 
                            Code and Dataset
                        </p>
                    </div>        
                    <hr class="my-1"/>
                </div>
            </div>
            <div class="row my-3">
                <div class="col-md-12">
                    <div class="card border-primary h-100">
                        <div class="card-body d-flex flex-column align-items-start">
                            As mentioned in the abstract of the submission, we open-source all codes and the dataset LAION-Audio-630K in the below links: <br>
                            (1) Code: https://github.com/LAION-AI/CLAP <br>
                            (2) LAION-Audio-630K: https://github.com/LAION-AI/audio-dataset <br>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="synthesis demo" class="pb-1">
        <div class="container" style="min-width:1300px;">
            <div class="row">
                <div class="col-md-12">
                    <div class="pb-1">
                        <p class="lead text-primary"> 
                            Details of LAION-Audio-630K
                        </p>
                    </div>        
                    <hr class="my-1"/>
                </div>
            </div>
            <div class="row my-3">
                <div class="col-md-12">
                    <div class="card border-primary h-100">
                        <div class="card-body d-flex flex-column align-items-start">
                            Regarding the section 2.1 and section 2.2 of the submisson: <br> 
                            (1) We list the specifications of website/sources from which we collect the audio samples and text captions for LAION-Audio-630K in Table 1. <br>
                            (2) We list the detail of three datasets in Table 2. We use the combination of them to train the model in the section 4 of the submission. <br>
                            <div class="row w-75 text-center my-3 align-items-center mx-auto">
                                <div class="col">
                                    <figure class="figure">
                                        <img src="files/dataset_detail.png" class="figure-img img-fluid rounded"  >
                                        <figcaption class="figure-caption text-center">Table 1: the composition of the proposed dataset LAION-Audio-630K.</figcaption>
                                    </figure>
                                </div>
                                <div class="col">
                                    <figure class="figure">
                                        <img src="files/trainingset_detail.png" class="figure-img img-fluid rounded" >
                                        <figcaption class="figure-caption text-center">Table 2: the detail of training datasets.</figcaption>
                                    </figure>
                                </div>
                            </div>
                            Regarding the section 3.4 of the submissin, we present the distribution of audio length on Epidemic Sound and Freesound, as parts of LAION-Audio-630K, to demonstrate the existence of variable-length problem in audio data processing and model training.
                            <div class="row w-75 text-center my-3 align-items-center mx-auto">
                                <div class="col">
                                    <figure class="figure">
                                        <img src="files/epidemic_duration_list.png" class="figure-img img-fluid rounded"  >
                                        <figcaption class="figure-caption text-center">Figure 1: the audio length distribution of Epidemic Sound.</figcaption>
                                    </figure>
                                </div>
                                <div class="col">
                                    <figure class="figure">
                                        <img src="files/freesound_test_duration_list.png" class="figure-img img-fluid rounded" >
                                        <figcaption class="figure-caption text-center">Figure 2: the audio length distribution of Freesound.</figcaption>
                                    </figure>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="synthesis demo" class="pb-1">
        <div class="container" style="min-width:1300px;">
            <div class="row">
                <div class="col-md-12">
                    <div class="pb-1">
                        <p class="lead text-primary"> 
                            Attentional Feature Fusion
                        </p>
                    </div>        
                    <hr class="my-1"/>
                </div>
            </div>
            <div class="row my-3">
                <div class="col-md-12">
                    <div class="card border-primary h-100">
                        <div class="card-body d-flex flex-column align-items-start ">
                            Regarding the section 3.4 of the submission, we demonstrate the "attentional feature fusion" architecture, a two-branch CNN network, to show how we combine the global information and the local information of input audios together. <br>
                            The fusion architecture acceptes two inputs: X is the global information, and Y is the merged local information. Two inputs are sent to two CNN networks to generate the coefficient, then X and Y are added by this coefficient.
                            <div class="row w-50 text-center my-3 align-items-center mx-auto">
                                <div class="col">
                                    <figure class="figure">
                                        <img src="files/concat_aff.png" class="figure-img img-fluid rounded"  >
                                        <figcaption class="figure-caption text-center">Figure 3: the attentional feature fusion architecture from the paper "Yimian Dai et al., Attentional Feature Fusion, WACV 2021."</figcaption>
                                    </figure>
                                </div>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="synthesis demo" class="pb-1">
        <div class="container" style="min-width:1300px;">
            <div class="row">
                <div class="col-md-12">
                    <div class="pb-1">
                        <p class="lead text-primary"> 
                            Examples of Keyword-to-Caption Augmentation
                        </p>
                    </div>        
                    <hr class="my-1"/>
                </div>
            </div>
            <div class="row my-3">
                <div class="col-md-12">
                    <div class="card border-primary h-100">
                        <div class="card-body d-flex flex-column align-items-start ">
                            Regarding the section 3.5 of the submission, we show some examples of keyword-to-caption by T5 model from AudioSet labels. And the de-biased version for the model training. <br> 
                            Additionally, when applying keyword to caption, we excluded samples shorter than 2 seconds, as we found in such case the audio is merely a single event, thus matching poorly with the caption generated.
                            When using keyword to caption in training dataset including audioset, we use only the captions generated by keyword to caption and exclude the captions generated by template. <br>

                            <div class="row w-75 text-center my-3 align-items-center mx-auto">
                                <div class="col">
                                    <figure class="figure">
                                        <img src="files/k2c_example.png" class="figure-img img-fluid rounded"  >
                                        <figcaption class="figure-caption text-center">Table 3: examples of keyword-to-caption augmentation from AudioSet labels and the de-biased version for the model training.</figcaption>
                                    </figure>
                                </div>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="synthesis demo" class="pb-1">
        <div class="container" style="min-width:1300px;">
            <div class="row">
                <div class="col-md-12">
                    <div class="pb-1">
                        <p class="lead text-primary"> 
                            Loss Trend of Different Audio/Text Encoder Combinations.
                        </p>
                    </div>        
                    <hr class="my-1"/>
                </div>
            </div>
            <div class="row my-3">
                <div class="col-md-12">
                    <div class="card border-primary h-100">
                        <div class="card-body d-flex flex-column align-items-start ">
                            Regarding to the section 4.2 of the submission, we conduct different combinations of audio encoders (PANN, HTSAT) and text encoders (CLIP Transformer, BERT, RoBERTa) to find the best basic setting of the contrastive language-audio pretraining model. <br>
                            The final decision is to use the RoBERTa, as we see the result on CLIP Transformer is not as good as the other two text encoders. We further visualize the loss trends of these three text encoders (+ PANN audio encoder) on the AudioCaps evaluation set below. <br>
                            We demonstrate that the worse result from CLIP Transformer is because of the overfitting issue, as the CLIP Transformer is trained by OPEN-AI's large-scale image-text dataset with the size of about 4 billion. This makes the audio encoder hard to learn a cross-modal representation because the text encoder is already very powerful. <br>
                            Compared to CLIP Transformer, RoBERTa and BERT seem to be a better choice as it has more generalization ability brought by the large-scale text pretraining instead of contrastive learning between text and image data. <br>
                            <div class="row w-50 text-center my-3 align-items-center mx-auto">
                                <div class="col">
                                    <figure class="figure">
                                        <img src="files/loss_curve.jpeg" class="figure-img img-fluid rounded"  >
                                        <figcaption class="figure-caption text-center">Figure 4: the evaluation loss trend of three text encoders (+ PANN encoder) on the AudioCaps evaluation set.</figcaption>
                                    </figure>
                                </div>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="synthesis demo" class="pb-1">
        <div class="container" style="min-width:1300px;">
            <div class="row">
                <div class="col-md-12">
                    <div class="pb-1">
                        <p class="lead text-primary"> 
                            Attentional Experiment of feature fusion on Freesound Dataset
                        </p>
                    </div>        
                    <hr class="my-1"/>
                </div>
            </div>
            <div class="row my-3">
                <div class="col-md-12">
                    <div class="card border-primary h-100">
                        <div class="card-body d-flex flex-column align-items-start ">
                            Regarding to the section 4.2 of the submission, to further evaluate the efficacy of feature fusion, apart from AudioCaps and Clotho datasets, we further evaluate our model on Freesound evaluation set, which contains more than 10-sec audio samples (similar to Clotho dataset). <br>
                            The result is shown in the below table, the notation is the same as the Table 3 in our submission paper. <br>
                            From this table, we can further prove that the feature fusion can improve the retrieval performance on the Freesound dataset. The performance on Freesound dataset shares a similar trend with that on Clotho dataset: <br>
                            (1) the performance trained on "AudioCaps + Clotho + LA." is better than that trained on "AudioCaps + Clotho + LA. + AudioSet". As demonstrate in the section 4.2, similar to Clotho, the Freesound dataset contains audio samples that are different from AudioSet, adding the AudioSet into the training will move the model's distribution 
                            out of general audio data to AudioSet-like audio data, such decreasing the perfomance. <br>
                            (2) the performance with feature fusion is better than that without feature fusion, as the Freesound dataset contains the samples larger than 10-secs, which is the same to Clotho dataset. 
                            Their performance trend are similar. <br>
                            <div class="row w-100 text-center my-3 align-items-center mx-auto">
                                <div class="col">
                                    <figure class="figure">
                                        <img src="files/freesound.PNG" class="figure-img img-fluid rounded"  >
                                        <figcaption class="figure-caption text-center">Table 4: the text-to-audio retrieval performance on Freesound evaluation set.</figcaption>
                                    </figure>
                                </div>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    

    <section id="synthesis demo" class="pb-1">
        <div class="container" style="min-width:1300px;">
            <div class="row">
                <div class="col-md-12">
                    <div class="pb-1">
                        <p class="lead text-primary"> 
                            Experiment Settings on Data Exclusion
                        </p>
                    </div>        
                    <hr class="my-1"/>
                </div>
            </div>
            <div class="row my-3">
                <div class="col-md-12">
                    <div class="card border-primary h-100">
                        <div class="card-body d-flex flex-column align-items-start ">
                            Regarding the section 4.3 of the submission, We excluded all the overlap samples and perform zero-shot evaluation on the whole remaining dataset. The below table shows the detail of it. <br> 
                            <div class="row w-50 text-center my-3 align-items-center mx-auto">
                                <div class="col">
                                    <figure class="figure">
                                        <img src="files/dataset_overlap.png" class="figure-img img-fluid rounded"  >
                                        <figcaption class="figure-caption text-center">Table 4: The overlaps between the training data and the zero-shot evaluation data, we excluded all these overlaps from the evalation sets to calculate the audio classification metrics.</figcaption>
                                    </figure>
                                </div>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section id="synthesis demo" class="pb-1">
        <div class="container" style="min-width:1300px;">
            <div class="row">
                <div class="col-md-12">
                    <div class="pb-1">
                        <p class="lead text-primary"> 
                            Acknowledgement
                        </p>
                    </div>        
                    <hr class="my-1"/>
                </div>
            </div>
            <div class="row my-3">
                <div class="col-md-12">
                    <div class="card border-primary h-100">
                        <div class="card-body d-flex flex-column align-items-start ">
                            Yusong Wu, Ke chen, Tianyu Zhang are opensource contributors to LAION projects. <br>
                            Our codebase is build on following open-source projects: 
                            <a href="https://github.com/qiuqiangkong/audioset_tagging_cnn">(1) PANN</a>
                            <a href="https://github.com/RetroCirce/HTS-Audio-Transformer">(2) HTSAT</a>
                            <a href="https://github.com/mlfoundations/open_clip">(3) open-clip</a>
                            <a href="https://pytorch.org/">(4) PyTorch</a>
                            We would like to thank the support of computation infrastructure from LAION, Stability.ai and Summit cluster from Oak Ridge National Laboratory. 
                            We would like to thank Christoph Schuhmann, Richard Vencu, Irina Rish, Romain Beaumon, as this project would not be possible without them. 
                            We would like to thank all the community contributors for contributing the collection of LAION-630k dataset. 
                            Those community contributors include but not limited to: [TODO Yuchen]. We would like to thank Xinhao Mei for explaining and helping on retrieval metrics.
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

</main>
    <footer id="footer" class="bg-dark text-light py-5">
    </footer> 
<script src="config/jquery.min.js"></script>
<script src="config/popper.min.js"></script>
<script src="config/bootstrap.min.js"></script>
<script src="config/scripts.js"></script>
</body>
</html>